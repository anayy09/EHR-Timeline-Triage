# Interpretability

## Overview

Interpretability is crucial for clinical AI systems. This document explains how EHR Timeline Triage generates human-readable explanations for its predictions.

## Design Philosophy

**Principles**:
1. **Transparency**: Show what the model used
2. **Localization**: Identify specific time periods and events
3. **Simplicity**: Use plain language, avoid jargon
4. **Honesty**: Include limitations and disclaimers

## Attribution Methods

### 1. Logistic Regression Attribution

**Method**: Linear coefficients

**Implementation**: `ehrtriage/explain/attribution.py::get_logistic_attributions`

**Formula**:
```
contribution_i = coefficient_i × feature_value_i
```

**Process**:
1. Extract model coefficients (learned weights)
2. Multiply by input feature values
3. Sort by absolute contribution
4. Return top k features

**Example**:
```python
[
    {
        "feature": "lactate_mean",
        "value": 3.5,
        "coefficient": 0.42,
        "contribution": 1.47  # positive = increases risk
    },
    {
        "feature": "heart_rate_max",
        "value": 125.0,
        "coefficient": 0.15,
        "contribution": 18.75
    },
    ...
]
```

**Interpretation**:
- **Positive contribution**: Feature increases risk
- **Negative contribution**: Feature decreases risk
- **Magnitude**: How much it matters

**Advantages**:
- Exact (not an approximation)
- Fast to compute
- Easy to understand

**Limitations**:
- No temporal information
- Assumes linear relationships
- Feature interactions not captured

### 2. Sequence Model Attribution

**Method**: Gradient-based temporal attribution

**Implementation**: `ehrtriage/explain/attribution.py::get_sequence_gradient_attribution`

**Process**:
1. Forward pass: input → prediction
2. Backward pass: compute ∇output/∇input
3. Aggregate gradient magnitude across features per timestep
4. Apply mask (ignore padding)
5. Normalize to sum to 1

**Formula**:
```
importance_t = Σ_d |∂ŷ/∂x_{t,d}| × mask_t
```

Where:
- t = timestep
- d = feature dimension
- ŷ = model output
- x = input sequence

**Output**: Array of importance scores [T]

**Example**:
```python
[0.05, 0.08, 0.12, 0.18, 0.22, 0.15, 0.10, 0.06, 0.03, 0.01, 0.0, 0.0]
# Timesteps 4-5 (hours 16-20) most important
```

**Top Timesteps**:
Extract k timesteps with highest importance:
```python
top_timesteps = [4, 5, 3]  # Indices
# Corresponds to hours [16-20, 20-24, 12-16]
```

**Feature Extraction**:
For each important timestep, identify top features:
```python
{
    "timestep": 4,
    "importance": 0.22,
    "features": [
        {"feature": "lactate", "value": 3.5},
        {"feature": "heart_rate", "value": 125},
        {"feature": "vasopressor", "value": 1.0}
    ]
}
```

**Advantages**:
- Temporal localization
- Works for any differentiable model
- Identifies critical periods

**Limitations**:
- Gradient can be noisy
- Approximation (not exact like logistic)
- Requires backpropagation

### Alternative Methods (Future)

**Integrated Gradients**:
- More stable than raw gradients
- Accumulates gradients along path from baseline

**Attention Weights** (Transformer):
- Direct interpretation from model
- Shows which timesteps model attends to
- Multi-head provides different perspectives

**SHAP Values**:
- Game-theoretic attribution
- Consistent and accurate
- Computationally expensive

## Natural Language Explanations

**Implementation**: `ehrtriage/explain/text_generator.py`

### Template-Based Generation

**Philosophy**: Deterministic, rule-based (no LLM)

**Reasons**:
- Reproducible
- Fast
- No external dependencies
- Fully controllable

### Logistic Regression Template

**Structure**:
```
{Risk Level} risk of {Outcome} (score: {X.XX}).

Key contributing factors:
  1. {Feature}: {Value} {direction} risk
  2. {Feature}: {Value} {direction} risk
  ...

[Disclaimer]
```

**Example Output**:
```
High risk of 30-day readmission (score: 0.78).

Key contributing factors:
  1. lactate (mean): 3.5 increases risk
  2. heart rate (max): 125.0 increases risk
  3. History of CHF increases risk
  4. creatinine (last): 2.1 increases risk
  5. age: 72 increases risk

Generated by a research model on deidentified or synthetic data.
Not for clinical use.
```

### Sequence Model Template

**Structure**:
```
{Risk Level} risk of {Outcome} (score: {X.XX}).

Key clinical signals during {time window}:

  1. Time period {hours}:
     - {Vital/Lab}: {Value}
     - {Vital/Lab}: {Value}
     - {Medication} administered

  2. Time period {hours}:
     ...

[Disclaimer]
```

**Example Output**:
```
High risk of 48-hour ICU mortality (score: 0.82).

Key clinical signals during first 48 hours of ICU stay:

  1. Time period hours 16-20:
     - Mean Arterial Pressure: 65.0
     - Lactate: 4.2
     - Vasopressor administered

  2. Time period hours 20-24:
     - Heart Rate: 125.0
     - Spo2: 88.0
     - Lactate: 5.1

  3. Time period hours 12-16:
     - Creatinine: 2.3
     - WBC: 18.5

Generated by a research model on deidentified or synthetic data.
Not for clinical use.
```

### Risk Level Categorization

**Thresholds**:
- **Low**: risk_score < 0.3
- **Medium**: 0.3 ≤ risk_score < 0.7
- **High**: risk_score ≥ 0.7

**Rationale**:
- Conservative for healthcare
- Adjustable based on operating point
- Consistent across models

## API Integration

### Contributing Events

**Format**:
```python
{
    "time": "Hour 16-20" | "overall",
    "type": "vital" | "lab" | "medication" | "feature",
    "code": "lactate" | "heart_rate",
    "value": 3.5,
    "contribution_score": 0.15
}
```

**Usage**: Frontend can highlight these events on timeline

### Explanation Text

**Purpose**: Display to user

**Requirements**:
- Clear and concise
- Factual (no speculation)
- Includes disclaimer

## Validation

### Sanity Checks

1. **Feature values are reasonable**: Within expected ranges
2. **Top features make clinical sense**: Known risk factors
3. **Temporal patterns are plausible**: Deterioration over time

### User Studies (Future)

- Clinician review of explanations
- Agreement with clinical reasoning
- Actionability assessment

## Limitations

### What Explanations Show

✓ Which features/times the model used
✓ Relative importance
✓ Direction of influence

### What Explanations DON'T Show

✗ Causal relationships
✗ Why a feature is important (domain knowledge)
✗ Uncertainty/confidence in attribution
✗ Counterfactuals ("what if")

### Known Issues

1. **Feature Interactions**: Linear attributions miss synergies
2. **Temporal Dependencies**: Hard to explain sequential relationships
3. **Confounding**: Model may use proxies, not true causes

## Disclaimer Requirements

**Always Include**:
```
Generated by a research model on deidentified or synthetic data.
Not for clinical use.
```

**Reasons**:
- Legal protection
- User awareness
- Ethical responsibility

**Placement**:
- API responses
- UI displays
- Printed reports

## Best Practices

### For Developers

1. **Validate attributions**: Check that they make sense
2. **Test edge cases**: Zero features, all padding, etc.
3. **Version explanations**: Track changes to templates
4. **Log explanations**: For audit and debugging

### For Users

1. **Read disclaimer**: Understand limitations
2. **Check raw data**: Don't rely only on explanation
3. **Consult domain experts**: Verify clinical plausibility
4. **Use as hypothesis**: Not as definitive answer

## Future Enhancements

### Counterfactual Explanations

**Concept**: "If lactate had been 2.0 instead of 4.5, risk would decrease to 0.35"

**Benefits**:
- Actionable insights
- Causal intuition

**Challenges**:
- Computationally expensive
- Unrealistic counterfactuals

### Confidence Intervals

**Concept**: "Risk score: 0.75 ± 0.10 (95% CI)"

**Benefits**:
- Uncertainty quantification
- More honest predictions

**Implementation**:
- Bootstrap sampling
- Bayesian neural networks

### Interactive Explanations

**Concept**: User can query model

Examples:
- "Why is lactate important?"
- "What if we remove this feature?"
- "Show similar past cases"

**Benefits**:
- Deeper understanding
- User engagement

**Challenges**:
- Complex UI
- Computational cost
